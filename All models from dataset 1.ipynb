{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15d2797-e313-47ae-a974-cd807a1e3c43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f749dc0c-a788-49c9-89d0-184789a29541",
   "metadata": {},
   "source": [
    "# **ANN + FINBERT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2220abf9-29ee-4a1d-a70f-8de059fdef36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üñ•Ô∏è Using device: cpu\n",
      "üìä Original Class Distribution:\n",
      "sentiment\n",
      "Neutral     2879\n",
      "Positive    1363\n",
      "Negative     604\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "‚úÖ Balanced Class Distribution:\n",
      "sentiment\n",
      "Negative    2879\n",
      "Positive    2879\n",
      "Neutral     2879\n",
      "Name: count, dtype: int64 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\VARSHINI.M\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\VARSHINI.M\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Train: 6045 | Val: 1296 | Test: 1296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 378/378 [09:31<00:00,  1.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìò EPOCH 1/10 SUMMARY\n",
      "Train‚Üí Acc: 69.98% | Loss: 0.7276\n",
      "Val  ‚Üí Acc: 75.39% | Loss: 0.6025\n",
      "Test ‚Üí Acc: 76.70% | Loss: 0.6071\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 378/378 [10:21<00:00,  1.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìò EPOCH 2/10 SUMMARY\n",
      "Train‚Üí Acc: 72.80% | Loss: 0.6540\n",
      "Val  ‚Üí Acc: 76.39% | Loss: 0.5941\n",
      "Test ‚Üí Acc: 77.16% | Loss: 0.5944\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10:  35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                            | 134/378 [02:55<05:11,  1.28s/it]"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Step 0: Imports & Setup\n",
    "# -------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    classification_report, accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    ")\n",
    "from sklearn.utils import resample\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"üñ•Ô∏è Using device:\", device)\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# -------------------------------\n",
    "# Step 1: Load Dataset\n",
    "# -------------------------------\n",
    "df = pd.read_csv(r\"C:\\Users\\VARSHINI.M\\OneDrive\\Desktop\\7th sem\\interdisclipinary\\Datasets\\Sentiment Analysis for Financial News\\all-data.csv\", encoding='latin-1', header=None)\n",
    "df.columns = ['sentiment', 'text']\n",
    "\n",
    "sentiment_map = {'negative': -1, 'neutral': 0, 'positive': 1}\n",
    "df['sentiment'] = df['sentiment'].map(sentiment_map)\n",
    "df.dropna(subset=['sentiment'], inplace=True)\n",
    "df['sentiment'] = df['sentiment'].astype(int)\n",
    "label_text_map = {-1: \"Negative\", 0: \"Neutral\", 1: \"Positive\"}\n",
    "\n",
    "print(\"üìä Original Class Distribution:\")\n",
    "print(df['sentiment'].map(label_text_map).value_counts(), \"\\n\")\n",
    "\n",
    "# -------------------------------\n",
    "# Step 1.5: Handle Imbalance\n",
    "# -------------------------------\n",
    "df_neg = df[df['sentiment'] == -1]\n",
    "df_neu = df[df['sentiment'] == 0]\n",
    "df_pos = df[df['sentiment'] == 1]\n",
    "max_size = max(len(df_neg), len(df_neu), len(df_pos))\n",
    "\n",
    "df_balanced = pd.concat([\n",
    "    resample(df_neg, replace=True, n_samples=max_size, random_state=42),\n",
    "    resample(df_neu, replace=True, n_samples=max_size, random_state=42),\n",
    "    resample(df_pos, replace=True, n_samples=max_size, random_state=42)\n",
    "]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(\"‚úÖ Balanced Class Distribution:\")\n",
    "print(df_balanced['sentiment'].map(label_text_map).value_counts(), \"\\n\")\n",
    "\n",
    "# -------------------------------\n",
    "# Step 2: Text Preprocessing\n",
    "# -------------------------------\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text)\n",
    "    text = re.sub(r'\\@\\w+|\\#', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    tokens = [lemmatizer.lemmatize(w) for w in text.split() if w not in stop_words]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "df_balanced['clean_text'] = df_balanced['text'].astype(str).apply(clean_text)\n",
    "\n",
    "# -------------------------------\n",
    "# Step 3: Data Split\n",
    "# -------------------------------\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    df_balanced['clean_text'], df_balanced['sentiment'],\n",
    "    test_size=0.3, stratify=df_balanced['sentiment'], random_state=42\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42\n",
    ")\n",
    "print(f\"üìö Train: {len(X_train)} | Val: {len(X_val)} | Test: {len(X_test)}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Step 4: Tokenization\n",
    "# -------------------------------\n",
    "tokenizer = BertTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "\n",
    "def encode_texts(texts, max_len=128):\n",
    "    return tokenizer(\n",
    "        texts.tolist(),\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_len,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "train_enc = encode_texts(X_train)\n",
    "val_enc = encode_texts(X_val)\n",
    "test_enc = encode_texts(X_test)\n",
    "\n",
    "sentiment_to_idx = {-1: 0, 0: 1, 1: 2}\n",
    "idx_to_label = {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}\n",
    "\n",
    "y_train_idx = y_train.map(sentiment_to_idx)\n",
    "y_val_idx = y_val.map(sentiment_to_idx)\n",
    "y_test_idx = y_test.map(sentiment_to_idx)\n",
    "\n",
    "# -------------------------------\n",
    "# Step 5: Dataset Class\n",
    "# -------------------------------\n",
    "class SentimentDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.encodings['input_ids'][idx],\n",
    "            'attention_mask': self.encodings['attention_mask'][idx],\n",
    "            'labels': self.labels[idx]\n",
    "        }\n",
    "\n",
    "train_ds = SentimentDataset(train_enc, torch.tensor(y_train_idx.values))\n",
    "val_ds = SentimentDataset(val_enc, torch.tensor(y_val_idx.values))\n",
    "test_ds = SentimentDataset(test_enc, torch.tensor(y_test_idx.values))\n",
    "\n",
    "# -------------------------------\n",
    "# Step 6: FinBERT + ANN Model\n",
    "# -------------------------------\n",
    "class FinBERT_ANN(nn.Module):\n",
    "    def __init__(self, hidden_dim=256, num_classes=3):\n",
    "        super(FinBERT_ANN, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"ProsusAI/finbert\")\n",
    "        self.fc1 = nn.Linear(self.bert.config.hidden_size, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        with torch.no_grad():\n",
    "            bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = bert_output.pooler_output\n",
    "        x = self.fc1(pooled_output)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        return self.fc2(x)\n",
    "\n",
    "model = FinBERT_ANN().to(device)\n",
    "\n",
    "# -------------------------------\n",
    "# Step 7: Training\n",
    "# -------------------------------\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=16, shuffle=False)\n",
    "test_loader = DataLoader(test_ds, batch_size=16, shuffle=False)\n",
    "\n",
    "train_accs, val_accs, test_accs = [], [], []\n",
    "train_losses, val_losses, test_losses = [], [], []\n",
    "\n",
    "def compute_accuracy_and_loss(loader):\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            ids = batch['input_ids'].to(device)\n",
    "            mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(ids, mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    acc = correct / total\n",
    "    return acc, total_loss / len(loader), np.array(all_labels), np.array(all_preds)\n",
    "\n",
    "EPOCHS = 10\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_train_loss, correct_train, total_train = 0, 0, 0\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
    "        optimizer.zero_grad()\n",
    "        ids = batch['input_ids'].to(device)\n",
    "        mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(ids, mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item()\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        correct_train += (preds == labels).sum().item()\n",
    "        total_train += labels.size(0)\n",
    "    train_acc = correct_train / total_train\n",
    "    train_loss = total_train_loss / len(train_loader)\n",
    "    val_acc, val_loss, _, _ = compute_accuracy_and_loss(val_loader)\n",
    "    test_acc, test_loss, _, _ = compute_accuracy_and_loss(test_loader)\n",
    "    train_accs.append(train_acc); val_accs.append(val_acc); test_accs.append(test_acc)\n",
    "    train_losses.append(train_loss); val_losses.append(val_loss); test_losses.append(test_loss)\n",
    "    print(f\"\\nüìò EPOCH {epoch+1}/{EPOCHS} SUMMARY\")\n",
    "    print(f\"Train‚Üí Acc: {train_acc*100:.2f}% | Loss: {train_loss:.4f}\")\n",
    "    print(f\"Val  ‚Üí Acc: {val_acc*100:.2f}% | Loss: {val_loss:.4f}\")\n",
    "    print(f\"Test ‚Üí Acc: {test_acc*100:.2f}% | Loss: {test_loss:.4f}\")\n",
    "    print(\"-\"*60)\n",
    "\n",
    "# -------------------------------\n",
    "# Step 8: Visualization\n",
    "# -------------------------------\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(range(EPOCHS), train_losses, label=\"Train Loss\")\n",
    "plt.plot(range(EPOCHS), val_losses, label=\"Val Loss\")\n",
    "plt.plot(range(EPOCHS), test_losses, label=\"Test Loss\")\n",
    "plt.title(\"Loss vs Epochs\"); plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(range(EPOCHS), train_accs, label=\"Train Acc\")\n",
    "plt.plot(range(EPOCHS), val_accs, label=\"Val Acc\")\n",
    "plt.plot(range(EPOCHS), test_accs, label=\"Test Acc\")\n",
    "plt.title(\"Accuracy vs Epochs\"); plt.xlabel(\"Epoch\"); plt.ylabel(\"Accuracy\"); plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# Step 9: Confusion Matrices\n",
    "# -------------------------------\n",
    "def plot_confusion_matrix(y_true, y_pred, title):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(cm, display_labels=[\"Negative\", \"Neutral\", \"Positive\"])\n",
    "    disp.plot(cmap=\"Blues\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "train_acc, _, y_train_true, y_train_pred = compute_accuracy_and_loss(train_loader)\n",
    "val_acc, _, y_val_true, y_val_pred = compute_accuracy_and_loss(val_loader)\n",
    "test_acc, _, y_test_true, y_test_pred = compute_accuracy_and_loss(test_loader)\n",
    "\n",
    "plot_confusion_matrix(y_train_true, y_train_pred, \"Train Confusion Matrix\")\n",
    "plot_confusion_matrix(y_val_true, y_val_pred, \"Validation Confusion Matrix\")\n",
    "plot_confusion_matrix(y_test_true, y_test_pred, \"Test Confusion Matrix\")\n",
    "\n",
    "# Overall Confusion Matrix\n",
    "y_all_true = np.concatenate([y_train_true, y_val_true, y_test_true])\n",
    "y_all_pred = np.concatenate([y_train_pred, y_val_pred, y_test_pred])\n",
    "plot_confusion_matrix(y_all_true, y_all_pred, \"Overall Confusion Matrix\")\n",
    "\n",
    "# -------------------------------\n",
    "# Step 10: Evaluation Report\n",
    "# -------------------------------\n",
    "print(\"\\nüìä FINAL TEST REPORT:\")\n",
    "print(classification_report(y_test_true, y_test_pred, target_names=[\"Negative\", \"Neutral\", \"Positive\"]))\n",
    "final_acc = accuracy_score(y_test_true, y_test_pred)\n",
    "print(f\"‚úÖ Final Test Accuracy: {final_acc*100:.2f}%\")\n",
    "\n",
    "# -------------------------------\n",
    "# Step 11: Save Model\n",
    "# -------------------------------\n",
    "torch.save(model.state_dict(), \"finbert_ann_balanced_model.pth\")\n",
    "print(\"‚úÖ Model saved as 'finbert_ann_balanced_model.pth'\")\n",
    "\n",
    "# -------------------------------\n",
    "# Step 12: Real-Time Prediction\n",
    "# -------------------------------\n",
    "def predict_sentiment(sentence):\n",
    "    model.eval()\n",
    "    cleaned = clean_text(sentence)\n",
    "    encoded = tokenizer(cleaned, return_tensors='pt', truncation=True, padding=True, max_length=128)\n",
    "    input_ids = encoded['input_ids'].to(device)\n",
    "    attn_mask = encoded['attention_mask'].to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids, attn_mask)\n",
    "        pred = torch.argmax(logits, dim=1).item()\n",
    "    label = idx_to_label[pred]\n",
    "    return f\"{label} ({['-1','0','1'][pred]})\"\n",
    "\n",
    "print(\"\\nüí¨ Example Predictions:\")\n",
    "examples = [\n",
    "    \"The company reported record profits this quarter!\",\n",
    "    \"The stock market is unstable due to new regulations.\",\n",
    "    \"Investors are unsure about the future of this firm.\"\n",
    "]\n",
    "for s in examples:\n",
    "    print(f\"{s} ‚Üí {predict_sentiment(s)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83128584-5023-44eb-bb3e-bd23be60af43",
   "metadata": {},
   "source": [
    "# **CNN + FINBERT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54d1c62-dea9-468c-851c-e14d8129d6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Step 0: Imports & Setup\n",
    "# -------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.utils import resample\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"üñ•Ô∏è Using device:\", device)\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# -------------------------------\n",
    "# Step 1: Load Dataset\n",
    "# -------------------------------\n",
    "df = pd.read_csv(\"/content/all-data.csv\", encoding='latin-1', header=None)\n",
    "df.columns = ['sentiment', 'text']\n",
    "\n",
    "sentiment_map = {'negative': -1, 'neutral': 0, 'positive': 1}\n",
    "df['sentiment'] = df['sentiment'].map(sentiment_map)\n",
    "df.dropna(subset=['sentiment'], inplace=True)\n",
    "df['sentiment'] = df['sentiment'].astype(int)\n",
    "label_text_map = {-1: \"Negative\", 0: \"Neutral\", 1: \"Positive\"}\n",
    "\n",
    "print(\"üìä Original Class Distribution:\")\n",
    "print(df['sentiment'].map(label_text_map).value_counts(), \"\\n\")\n",
    "\n",
    "# -------------------------------\n",
    "# Step 1.5: Handle Imbalance\n",
    "# -------------------------------\n",
    "df_neg = df[df['sentiment'] == -1]\n",
    "df_neu = df[df['sentiment'] == 0]\n",
    "df_pos = df[df['sentiment'] == 1]\n",
    "\n",
    "max_size = max(len(df_neg), len(df_neu), len(df_pos))\n",
    "df_neg_over = resample(df_neg, replace=True, n_samples=max_size, random_state=42)\n",
    "df_neu_over = resample(df_neu, replace=True, n_samples=max_size, random_state=42)\n",
    "df_pos_over = resample(df_pos, replace=True, n_samples=max_size, random_state=42)\n",
    "\n",
    "df_balanced = pd.concat([df_neg_over, df_neu_over, df_pos_over])\n",
    "df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(\"‚úÖ Balanced Class Distribution:\")\n",
    "print(df_balanced['sentiment'].map(label_text_map).value_counts(), \"\\n\")\n",
    "\n",
    "# -------------------------------\n",
    "# Step 2: Text Preprocessing\n",
    "# -------------------------------\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text)\n",
    "    text = re.sub(r'\\@\\w+|\\#', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    tokens = [lemmatizer.lemmatize(w) for w in text.split() if w not in stop_words]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "df_balanced['clean_text'] = df_balanced['text'].astype(str).apply(clean_text)\n",
    "\n",
    "# -------------------------------\n",
    "# Step 3: Data Split\n",
    "# -------------------------------\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    df_balanced['clean_text'], df_balanced['sentiment'],\n",
    "    test_size=0.3, stratify=df_balanced['sentiment'], random_state=42\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"üìö Train: {len(X_train)} | Val: {len(X_val)} | Test: {len(X_test)}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Step 4: Tokenization\n",
    "# -------------------------------\n",
    "tokenizer = BertTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "\n",
    "def encode_texts(texts, max_len=128):\n",
    "    return tokenizer(\n",
    "        texts.tolist(),\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_len,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "train_enc = encode_texts(X_train)\n",
    "val_enc = encode_texts(X_val)\n",
    "test_enc = encode_texts(X_test)\n",
    "\n",
    "sentiment_to_idx = {-1: 0, 0: 1, 1: 2}\n",
    "idx_to_sentiment = {0: -1, 1: 0, 2: 1}\n",
    "idx_to_label = {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}\n",
    "\n",
    "y_train_idx = y_train.map(sentiment_to_idx)\n",
    "y_val_idx = y_val.map(sentiment_to_idx)\n",
    "y_test_idx = y_test.map(sentiment_to_idx)\n",
    "\n",
    "# -------------------------------\n",
    "# Step 5: Dataset Class\n",
    "# -------------------------------\n",
    "class SentimentDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.encodings['input_ids'][idx],\n",
    "            'attention_mask': self.encodings['attention_mask'][idx],\n",
    "            'labels': self.labels[idx]\n",
    "        }\n",
    "\n",
    "train_ds = SentimentDataset(train_enc, torch.tensor(y_train_idx.values))\n",
    "val_ds = SentimentDataset(val_enc, torch.tensor(y_val_idx.values))\n",
    "test_ds = SentimentDataset(test_enc, torch.tensor(y_test_idx.values))\n",
    "\n",
    "# -------------------------------\n",
    "# Step 6: FinBERT + CNN MODEL\n",
    "# -------------------------------\n",
    "class FinBERT_CNN(nn.Module):\n",
    "    def __init__(self, num_classes=3, num_filters=128, filter_sizes=[2, 3, 4]):\n",
    "        super(FinBERT_CNN, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"ProsusAI/finbert\")\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=self.bert.config.hidden_size,\n",
    "                      out_channels=num_filters,\n",
    "                      kernel_size=k)\n",
    "            for k in filter_sizes\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc = nn.Linear(num_filters * len(filter_sizes), num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        with torch.no_grad():  # Freeze FinBERT\n",
    "            bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        x = bert_output.last_hidden_state.transpose(1, 2)\n",
    "        conv_results = [torch.relu(conv(x)) for conv in self.convs]\n",
    "        pooled = [torch.max(c, dim=2)[0] for c in conv_results]\n",
    "        cat = torch.cat(pooled, dim=1)\n",
    "        out = self.dropout(cat)\n",
    "        return self.fc(out)\n",
    "\n",
    "model = FinBERT_CNN().to(device)\n",
    "\n",
    "# -------------------------------\n",
    "# Step 7: Training\n",
    "# -------------------------------\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=16, shuffle=False)\n",
    "test_loader = DataLoader(test_ds, batch_size=16, shuffle=False)\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "train_accuracies, val_accuracies = [], []\n",
    "\n",
    "def compute_accuracy_and_loss(loader):\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    preds_all, labels_all = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            ids = batch['input_ids'].to(device)\n",
    "            mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(ids, mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            preds_all.extend(preds.cpu().numpy())\n",
    "            labels_all.extend(labels.cpu().numpy())\n",
    "    return correct / total, total_loss / len(loader), preds_all, labels_all\n",
    "\n",
    "EPOCHS = 10\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_train_loss, correct_train, total_train = 0, 0, 0\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
    "        optimizer.zero_grad()\n",
    "        ids = batch['input_ids'].to(device)\n",
    "        mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(ids, mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item()\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        correct_train += (preds == labels).sum().item()\n",
    "        total_train += labels.size(0)\n",
    "\n",
    "    train_acc = correct_train / total_train\n",
    "    train_loss = total_train_loss / len(train_loader)\n",
    "    val_acc, val_loss, _, _ = compute_accuracy_and_loss(val_loader)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    val_accuracies.append(val_acc)\n",
    "\n",
    "    print(f\"\\nüìò EPOCH {epoch+1}/{EPOCHS} SUMMARY\")\n",
    "    print(f\"Train  ‚Üí Acc: {train_acc*100:.2f}% | Loss: {train_loss:.4f}\")\n",
    "    print(f\"Val    ‚Üí Acc: {val_acc*100:.2f}% | Loss: {val_loss:.4f}\")\n",
    "    print(\"-\"*60)\n",
    "\n",
    "# -------------------------------\n",
    "# Step 8: Accuracy & Loss Plots\n",
    "# -------------------------------\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(val_losses, label=\"Validation Loss\")\n",
    "plt.title(\"Loss vs Epochs\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(train_accuracies, label=\"Train Accuracy\")\n",
    "plt.plot(val_accuracies, label=\"Validation Accuracy\")\n",
    "plt.title(\"Accuracy vs Epochs\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# Step 9: Confusion Matrices\n",
    "# -------------------------------\n",
    "def plot_conf_matrix(y_true, y_pred, title):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(6,5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=[\"Negative\", \"Neutral\", \"Positive\"],\n",
    "                yticklabels=[\"Negative\", \"Neutral\", \"Positive\"])\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.show()\n",
    "\n",
    "train_acc, _, y_pred_train, y_true_train = compute_accuracy_and_loss(train_loader)\n",
    "val_acc, _, y_pred_val, y_true_val = compute_accuracy_and_loss(val_loader)\n",
    "test_acc, _, y_pred_test, y_true_test = compute_accuracy_and_loss(test_loader)\n",
    "\n",
    "print(\"\\nüìä Confusion Matrices:\")\n",
    "plot_conf_matrix(y_true_train, y_pred_train, \"Train Confusion Matrix\")\n",
    "plot_conf_matrix(y_true_val, y_pred_val, \"Validation Confusion Matrix\")\n",
    "plot_conf_matrix(y_true_test, y_pred_test, \"Test Confusion Matrix\")\n",
    "\n",
    "# Overall CM\n",
    "y_true_all = y_true_train + y_true_val + y_true_test\n",
    "y_pred_all = y_pred_train + y_pred_val + y_pred_test\n",
    "plot_conf_matrix(y_true_all, y_pred_all, \"Overall Confusion Matrix\")\n",
    "\n",
    "# -------------------------------\n",
    "# Step 10: Final Evaluation\n",
    "# -------------------------------\n",
    "def evaluate_model(model, dataset, y_true_idx):\n",
    "    loader = DataLoader(dataset, batch_size=32)\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            ids = batch['input_ids'].to(device)\n",
    "            mask = batch['attention_mask'].to(device)\n",
    "            outputs = model(ids, mask)\n",
    "            preds.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
    "    y_true_text = [idx_to_label[i] for i in y_true_idx]\n",
    "    y_pred_text = [idx_to_label[i] for i in preds]\n",
    "    print(\"\\nüìä FINAL TEST REPORT:\")\n",
    "    print(classification_report(y_true_text, y_pred_text))\n",
    "    return accuracy_score(y_true_idx, preds)\n",
    "\n",
    "final_acc = evaluate_model(model, test_ds, y_test_idx)\n",
    "print(f\"‚úÖ Final Test Accuracy: {final_acc*100:.2f}%\")\n",
    "\n",
    "# -------------------------------\n",
    "# Step 11: Save Model\n",
    "# -------------------------------\n",
    "torch.save(model.state_dict(), \"finbert_cnn_balanced_model.pth\")\n",
    "print(\"‚úÖ Model saved as 'finbert_cnn_balanced_model.pth'\")\n",
    "\n",
    "# -------------------------------\n",
    "# Step 12: Real-Time Prediction\n",
    "# -------------------------------\n",
    "def predict_sentiment(sentence):\n",
    "    model.eval()\n",
    "    cleaned = clean_text(sentence)\n",
    "    encoded = tokenizer(cleaned, return_tensors='pt', truncation=True, padding=True, max_length=128)\n",
    "    input_ids = encoded['input_ids'].to(device)\n",
    "    attn_mask = encoded['attention_mask'].to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids, attn_mask)\n",
    "        pred = torch.argmax(logits, dim=1).item()\n",
    "    sentiment_val = idx_to_sentiment[pred]\n",
    "    return f\"{idx_to_label[pred]} ({sentiment_val})\"\n",
    "\n",
    "print(\"\\nüí¨ Example Predictions:\")\n",
    "examples = [\n",
    "    \"The company reported record profits this quarter!\",\n",
    "    \"The stock market is unstable due to new regulations.\",\n",
    "    \"Investors are unsure about the future of this firm.\"\n",
    "]\n",
    "for s in examples:\n",
    "    print(f\"{s} ‚Üí {predict_sentiment(s)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b92537-29ba-48ee-987e-644b6df00de9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
